# RLS (Rate Limit Service) Values - Production Configuration
# Use this file for deploying RLS in production with optimized settings

# Service configuration
service:
  type: ClusterIP
  port: 8082  # Main service port (admin API)
  ports:
    extAuthz: 8080
    rateLimit: 8081
    admin: 8082
    metrics: 9090

# Tenant identification
tenantHeader: "X-Scope-OrgID"

# Limits configuration - Production Optimized
limits:
  maxRequestBytes: "10485760"  # 10 MiB - Increased for large payloads
  maxBodyBytes: 1048576        # 1 MiB - Reasonable body size limit
  failureModeAllow: true       # Production: Allow requests when parsing fails
  defaultSamplesPerSecond: 1000      # 1000 samples/sec - Enable rate limiting
  defaultBurstPercent: 20      # 20% burst allowance
  enforceBodyParsing: true     # Production: Enable body parsing for cardinality analysis
  defaultMaxLabelsPerSeries: 10      # Reasonable label limit
  defaultMaxLabelValueLength: 2048   # Reasonable label value length
  defaultMaxSeriesPerRequest: 10000  # Reasonable series per request limit

# Enforcement Configuration - Production Ready
enforcement:
  enabled: true
  # Cardinality limits - ENABLED
  enforceMaxSeriesPerRequest: true   # per_user_series_limit
  enforceMaxSeriesPerMetric: true    # per_metric_series_limit
  enforceMaxLabelsPerSeries: true    # per_labels_per_series_limit
  # Rate limiting - ENABLED
  enforceSamplesPerSecond: true      # Enable ingestion rate limiting
  enforceBytesPerSecond: false       # No bytes rate limiting (focus on samples)
  # Body size - ENABLED
  enforceMaxBodyBytes: true          # Enable body size enforcement

# ðŸ”§ NEW: New series leniency configuration
newTenantLeniency: true  # Enable lenient limits (50%) when adding new series to prevent false positives

# Store configuration
store:
  backend: "redis"  # Use Redis for production (shared state across pods)

# Redis configuration for external Redis service
redis:
  enabled: true
  mode: "external"  # Use external Redis service
  external:
    address: "redis-shared-service.redis-shared.svc.cluster.local:6379"

# Mimir configuration for direct integration
mimir:
  host: "mock-mimir-distributor.mimir.svc.cluster.local"
  port: "8080"

# Logging configuration - PRODUCTION OPTIMIZED
log:
  level: "info"  # Production logging level
  enableGRPCLogs: false  # Disable gRPC logs for performance
  enableDetailedLogs: false  # Disable detailed logs for performance

# Performance tuning - HIGH SCALE OPTIMIZED
performance:
  # Request processing optimization
  maxConcurrentRequests: 5000    # High scale: 5K concurrent requests for 50 tenants
  requestTimeout: "60s"          # High scale: Increased timeout for large payloads
  bodyParseTimeout: "20s"        # High scale: Increased timeout for 5M series parsing
  
  # Redis connection optimization (High scale optimized)
  redisPoolSize: 100             # High scale: Larger pool for 10 pods
  redisMinIdleConns: 20          # High scale: More idle connections
  redisMaxRetries: 5             # High scale: More retries for reliability
  redisConnectTimeout: "2s"      # High scale: Increased connection timeout
  redisReadTimeout: "5s"         # High scale: Increased read timeout
  redisWriteTimeout: "5s"        # High scale: Increased write timeout
  
  # Memory optimization
  enableMemoryOptimization: true # Enable memory-efficient processing
  maxRequestBodySize: "10MB"     # Maximum request body size
  enableRequestPooling: true     # Enable request object pooling
  enableMetricsOptimization: true # Optimize metrics collection
  
  # Circuit breaker settings (Production)
  circuitBreakerEnabled: true    # Enable circuit breaker for reliability
  circuitBreakerThreshold: 5     # Failure threshold
  circuitBreakerTimeout: "30s"   # Recovery timeout

# Deployment configuration - SCALE OPTIMIZED
replicaCount: 10  # High scale: 50 tenants, 5M series - increased for reliability

image:
  repository: ghcr.io/akshaydubey29/mimir-rls
  tag: "latest"  # Latest version with scientific notation support and multi-arch builds
  pullPolicy: IfNotPresent  # Production: Use IfNotPresent for efficiency

# Resource configuration - HIGH SCALE OPTIMIZED
resources:
  limits:
    cpu: 2000m    # High scale: 2 cores for 5M series processing
    memory: 2Gi   # High scale: 2GB for large tenant data
  requests:
    cpu: 500m     # High scale: 0.5 cores minimum
    memory: 1Gi   # High scale: 1GB minimum for tenant caching

# Security context
securityContext:
  allowPrivilegeEscalation: false
  readOnlyRootFilesystem: true
  runAsNonRoot: true
  runAsUser: 1000
  capabilities:
    drop:
      - ALL

# Horizontal Pod Autoscaler (HIGH SCALE OPTIMIZED)
hpa:
  enabled: true
  minReplicas: 8    # High scale: Minimum 8 replicas for 50 tenants
  maxReplicas: 20   # High scale: Maximum 20 replicas for peak load
  targetCPUUtilizationPercentage: 60    # Lower threshold for faster scaling
  targetMemoryUtilizationPercentage: 60 # Lower threshold for faster scaling

# Pod Disruption Budget - HIGH SCALE
pdb:
  enabled: true
  minAvailable: 6  # High scale: Ensure at least 6 pods are always available

# Service Account
serviceAccount:
  create: true
  annotations: {}

# Pod annotations - Production
podAnnotations:
  prometheus.io/scrape: "true"
  prometheus.io/port: "9090"
  prometheus.io/path: "/metrics"

# Node selector
nodeSelector: {}

# Tolerations
tolerations: []

# Affinity - Production ready
affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 100
      podAffinityTerm:
        labelSelector:
          matchExpressions:
          - key: app.kubernetes.io/name
            operator: In
            values:
            - mimir-rls
        topologyKey: kubernetes.io/hostname

# Service Monitor for Prometheus - Production
serviceMonitor:
  enabled: false  # Disable ServiceMonitor to avoid CRD issues
  scrapeTimeout: 10s
  labels: {}
  annotations: {}

# Liveness and readiness probes - HIGH SCALE
livenessProbe:
  httpGet:
    path: /readyz
    port: admin
  initialDelaySeconds: 60  # High scale: Longer initial delay for large tenant loading
  periodSeconds: 30        # High scale: Less frequent checks to reduce overhead
  timeoutSeconds: 10       # High scale: Longer timeout for health checks
  failureThreshold: 5      # High scale: More failures before restart

readinessProbe:
  httpGet:
    path: /readyz
    port: admin
  initialDelaySeconds: 45  # High scale: Allow time for tenant data loading
  periodSeconds: 15        # High scale: More frequent readiness checks
  timeoutSeconds: 8        # High scale: Longer timeout
  failureThreshold: 3      # High scale: Fewer failures before marking not ready
  successThreshold: 2      # High scale: Require 2 successful checks before ready

# Startup probe - HIGH SCALE
startupProbe:
  httpGet:
    path: /readyz
    port: admin
  initialDelaySeconds: 30  # High scale: Allow time for initial tenant loading
  periodSeconds: 15        # High scale: Less frequent during startup
  timeoutSeconds: 10       # High scale: Longer timeout during startup
  failureThreshold: 40     # High scale: More failures allowed during startup (10 minutes total)
